{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Week 6. NLP and Imported Text Files(Template).</h2><br><br>\n",
    "    <p>In your groups, upload the .txt file to the JN directory and go through each step below. As you work out each coding solution, discuss your choices in your groups.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Import NLTK</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nltk package \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Import a .txt file</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open() an instance of the text .txt file\n",
    "\n",
    "# read() the text file into the coding environment \n",
    "\n",
    "\n",
    "# print() out the variable to which you assigned the text string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Convert to lowercases</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the imported text to .lower() cases.\n",
    "\n",
    "# print() out the variable to which you assigned the converted string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Tokenize</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up the text string into a list of word strings with .word_tokenize().\n",
    "\n",
    "\n",
    "# print() out the tokenized list of word strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Lemmatize</h3>\n",
    "       \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import WrodNetLemmatizer from the stem sub-library of the nltk package.\n",
    "\n",
    "\n",
    "# Create an instance of the WordNet lemmatizer and store it in a variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the for-loop run through the tokenized list and lemmatize words.\n",
    "# Your loop should store the lemmatized words in an empty list.\n",
    "\n",
    "\n",
    "# Print out the list to check whether the loop succeeded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Clean Data from Stop Words</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords object from the corpus sub-library of NLTK.\n",
    "\n",
    "\n",
    "\n",
    "# Store the stopword object for the English language in a new variable.\n",
    "# Don't forget to pass the stopword object to the set() function while assigning it to the variable. \n",
    "\n",
    "\n",
    "# print() out the variable to check the content of stopwords. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a for-loop or a list comprehension construct to remove stopwords from the lemmatized dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print() out the new variable to inspect the result of cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Compute Word Frequencies</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the probability sub-library of the nltk library, import the FreqDist function.\n",
    "\n",
    "\n",
    "# Pass the variable created in the code cell above to the FreqDist() function.\n",
    "\n",
    "\n",
    "# print() out the new variable which should return a dictionary structure with words and frequency counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Visualize the Distribution of Word Frequencies</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot sub-library of the matplotlib library under some alias.\n",
    "\n",
    "\n",
    "# Declare a magic line to show visuals inline this file.\n",
    "\n",
    "\n",
    "# Visualize the frequency distributions stored in the dictionary created in the code cell above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Iterate Data Cleaning</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the partially cleaned dataset and find other items that need to be removed, e.g. punctuation marks.\n",
    "# Create a custom list of your own stop words.\n",
    "\n",
    "\n",
    "# print() out the new variable in which your custom list is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write a for-loop or a list comprehension construct to remove stopwords from the lemmatized dataset.\n",
    "# The loop/construct should evaluate each item in the dataset against items in your custom list.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print() out the new variable to inspect the results of the second round of cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the variable created in the code cell above to the FreqDist() function.\n",
    "\n",
    "\n",
    "# print() out the new variable which should return a dictionary structure with words and frequency counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the magic line to show visuals inline this file.\n",
    "\n",
    "\n",
    "# Visualize the distribution of word frequencies fromn the dataset created in the code above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With .most_common()sample 10 to 20 top words of the cleaned text from the variable you have justed created above.\n",
    "\n",
    "\n",
    "# print() out the new variable which should return\n",
    "# a list with 10-20 tuples each of which should contain 2 values: string (word) and number (frequency).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Convert to a pandas' DataFrame</h3>\n",
    "       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas module under some alias.\n",
    "\n",
    "\n",
    "#Convert the list with tuples created above to a pandas' DataFrame.\n",
    "\n",
    "\n",
    "# print() out the new variable that contains the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Visualize a DataFrame</h3>       \n",
    "<p> </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the first and second columns of the DataFrame as a bar chart with the matplotlib.pyplot. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Generate Concordances</h3>\n",
    "    <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Text from the text sub-library of NLTK.\n",
    "# You will need it to convert a cleaned text to a Text object so that you could generate a concordance view.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Text object from a cleaned list of words stored somewhere above.\n",
    "\n",
    "\n",
    "\n",
    "# print() out the variable you've just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a concordance viewer with .concordance() for 4 specific words from the text you are dealing with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Create a Dispersion Plot</h3>\n",
    "    <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .dispersion_plot() for several words to see how some top frequent words are spread across the text.\n",
    "\n",
    "\n",
    "\n",
    "# print() out the variable to see the resulting visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <h3>Write (Export) a .txt file</h3>\n",
    "    <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackberrying sylvia plath nobody lane nothing nothing blackberry blackberry either side though right mainly blackberry alley going hook sea somewhere end heaving blackberry big ball thumb dumb eye ebon hedge fat blue-red juice squander finger asked blood sisterhood must love accommodate milkbottle flattening side overhead go chough black cacophonous flocks— bit burnt paper wheeling blown sky voice protesting protesting think sea appear high green meadow glowing lit within come one bush berry ripe bush fly hanging bluegreen belly wing pane chinese screen honey-feast berry ha stunned believe heaven one hook berry bush end thing come sea two hill sudden wind funnel slapping phantom laundry face hill green sweet tasted salt follow sheep path last hook brings hill ’ northern face face orange rock look nothing nothing great space white pewter light din like silversmith beating beating intractable metal\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Let's put a list of cleaned word strings back into one long string with the string method .join()\n",
    "\n",
    "\n",
    "\n",
    "# print() out the string object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new string data to an external .txt file.\n",
    "# First, with open() create an instance of a new .txt file: \n",
    "# give it a name and pass the argument \"w\" for writing mode.\n",
    "\n",
    "\n",
    "\n",
    "# Then write the data into that file with .write()\n",
    "\n",
    "\n",
    "# Finally, close the file with .close() and check in the directory whether the file is there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>Homework 6</h2><br>\n",
    "<p>\n",
    "If you don't finish all the steps in class, it will be your homework to complete the tasks in this file on your own. If your group completes the assignment in class, then choose another text to process, as above, for your homework.</p>\n",
    "<p>\n",
    "In either case, on top of the coding tasks, write a paragraph or two to offer some insight into the style and / or meaning of the text based on your findings about natural language data. Consider top frequencies (What can you tell about the dominating imagery, motifs and the like in this text?); the distribution of function vs. content words (What can you tell about the meaning and / or style of the text from the type of content words and from the type of functions words used?); concordances (What patterns of word use do you observe in a concordancer? How key words behave in context? How individual patterns contribute to the overall meaning of the text?).</p>\n",
    "<p>\n",
    "Add a markdown cell below to enter your reflections.</p><br>\n",
    "<p>Submit the completed Jupyter Notebook as 'homework6.ipynb' via the designated area called 'Homework 6' under Week 6 on KEATS by 4pm on <b>Wed 25th November 2020</b>.</p><br>    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
